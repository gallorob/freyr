{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Load all results and set up main variables",
   "id": "2041814c96aefed8"
  },
  {
   "cell_type": "code",
   "source": [
    "results_fname = 'summary_results.csv'\n",
    "\n",
    "freyr_bootrstrap_results = pd.read_csv(os.path.join('./experiments/freyr_bootstrap', results_fname))\n",
    "freyr_nobootstrap_results = pd.read_csv(os.path.join('./experiments/freyr_no_bootstrap', results_fname))\n",
    "tool_bootrstrap_results = pd.read_csv(os.path.join('experiments/tool_v2_bootstrap', results_fname))\n",
    "tool_nobootstrap_results = pd.read_csv(os.path.join('./experiments/tool_v2_no_bootstrap', results_fname))\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ee85dbb198edd0b6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "freyr_bootrstrap_results['mode'] = 'freyr'\n",
    "freyr_bootrstrap_results['bootstrap'] = True\n",
    "\n",
    "freyr_nobootstrap_results['mode'] = 'freyr'\n",
    "freyr_nobootstrap_results['bootstrap'] = False\n",
    "\n",
    "tool_bootrstrap_results['mode'] = 'tool'\n",
    "tool_bootrstrap_results['bootstrap'] = True\n",
    "\n",
    "tool_nobootstrap_results['mode'] = 'tool'\n",
    "tool_nobootstrap_results['bootstrap'] = False\n",
    "\n",
    "# Concatenate all dataframes\n",
    "df = pd.concat([\n",
    "    freyr_bootrstrap_results,\n",
    "    freyr_nobootstrap_results,\n",
    "    # tool_bootrstrap_results,\n",
    "    # tool_nobootstrap_results\n",
    "], ignore_index=True)\n",
    "\n",
    "# df = df.loc[((df['intent_llm'] == 'command-r') | (df['intent_llm'] == 'llama3.1') | (df['intent_llm'] == 'qwen2.5')) &\n",
    "# \t                ((df['params_llm'] == df['intent_llm']) | (pd.isna(df['params_llm'])))]"
   ],
   "id": "31fce283b22bb1f4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from typing import Tuple\n",
    "\n",
    "\n",
    "def analyze_tokens_and_time(df: pd.DataFrame,\n",
    "                            step_n: int,\n",
    "                            freyr_mode: bool,\n",
    "                            with_bootstrap: bool) -> Tuple[int, int, float]:\n",
    "\twith open(f'./experiments/{\"freyr\" if freyr_mode else \"tool_v2\"}_{\"bootstrap\" if with_bootstrap else \"no_bootstrap\"}/{df[\"logfile\"]}.log', 'r') as f:\n",
    "\t\tls = f.readlines()\n",
    "\n",
    "\t\tstep_start_idxs = [i for i, l in enumerate(ls) if 'main - step=' in l]\n",
    "\n",
    "\t\tin_tks_step, out_tks_step, t_step = 0, 0, 0.0\n",
    "\t\tfor j, l in enumerate(ls[step_start_idxs[step_n - 1]:]):\n",
    "\t\t\t# LocalLLM.extract_intents - Prompt Tokens: 448; Completion Tokens: 6; Time: 0.2812\n",
    "\t\t\tif 'Prompt Tokens' in l:\n",
    "\t\t\t\tin_tks, out_tks, t = l.split(' - ')[1].split(';')\n",
    "\t\t\t\tin_tks_step += int(in_tks.split(':')[1])\n",
    "\t\t\t\tout_tks_step += int(out_tks.split(':')[1])\n",
    "\t\t\t\tt_step += float(t.split(':')[1])\n",
    "\t\t\t\t# print(step_n, j, l)\n",
    "\n",
    "\t\t\t# print('\\t', step_n, in_tks_step, out_tks_step, t_step)\n",
    "\t\t\t# print('\\t', len(step_start_idxs), step_n < len(step_start_idxs) - 1, j, step_start_idxs[step_n - 1] + j > step_start_idxs[step_n])\n",
    "\t\t\tif step_n < len(step_start_idxs) and step_start_idxs[step_n - 1] + j > step_start_idxs[step_n]:\n",
    "\t\t\t\tbreak\n",
    "\n",
    "\treturn in_tks_step, out_tks_step, t_step"
   ],
   "id": "bb8e4c580491cddf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "full_df = pd.DataFrame()\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "\tin_tks, out_tks, t = analyze_tokens_and_time(row, row['step'], row['mode'] == 'freyr', row['bootstrap'])\n",
    "\t# if row['step'] == 12: raise Exception\n",
    "\tfull_df = pd.concat([full_df, pd.DataFrame({\n",
    "\t\t**row.to_dict(),\n",
    "\t\t'in_tks': in_tks,\n",
    "\t\t'out_tks': out_tks,\n",
    "\t\t't_step': t,\n",
    "\t}, index=[0])], ignore_index=True)"
   ],
   "id": "350baa83212f3dc0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "full_df.to_csv('./experiments/full_freyr_results.csv', index=False)",
   "id": "9653c3b4b4a3162d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "llms = freyr_bootrstrap_results['intent_llm'].unique()\n",
    "tcases = freyr_bootrstrap_results['test_case'].unique()\n",
    "steps_per_tcase = {\n",
    "\t1: 7,\n",
    "\t2: 9,\n",
    "\t3: 10,\n",
    "\t4: 11,\n",
    "\t5: 13\n",
    "}"
   ],
   "id": "7000dfeb0a96a949",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from typing import Tuple\n",
    "\n",
    "\n",
    "def get_credible_interval(s: pd.Series) -> Tuple[float, float]:\n",
    "\tm, c, s = s.mean(), s.count(), s.std()\n",
    "\treturn m + 1.96 * s / np.sqrt(c), m - 1.96 * s / np.sqrt(c)\n",
    "\n",
    "\n",
    "def get_summary_freyr(df: pd.DataFrame,\n",
    "                      with_bootstrap: bool) -> pd.DataFrame:\n",
    "\tresults_summary = pd.DataFrame()\n",
    "\tfor intent_llm in llms:\n",
    "\t\tfor params_llm in llms:\n",
    "\t\t\tfor tcase in tcases:\n",
    "\t\t\t\ttcase_results = pd.DataFrame()\n",
    "\t\t\t\tfor run_n in df['run_n'].unique():\n",
    "\t\t\t\t\ttest_results = df.loc[(df['intent_llm'] == intent_llm) &\n",
    "\t\t\t\t\t                      (df['params_llm'] == params_llm) &\n",
    "\t\t\t\t\t                      (df['run_n'] == run_n) &\n",
    "\t\t\t\t\t                      (df['test_case'] == tcase)]\n",
    "\t\t\t\t\tif not test_results.empty:\n",
    "\t\t\t\t\t\tif not with_bootstrap:\n",
    "\t\t\t\t\t\t\tvalid_steps = test_results[test_results['valid_design'] == True]\n",
    "\t\t\t\t\t\t\tn_steps = valid_steps['step'].max() if not valid_steps.empty else 0\n",
    "\t\t\t\t\t\telse:\n",
    "\t\t\t\t\t\t\tn_steps = df['step'].max()\n",
    "\t\t\t\t\t\tn_valid = len(test_results[test_results['valid_design'] == True])\n",
    "\t\t\t\t\t\ttcase_results = pd.concat([tcase_results, pd.DataFrame({\n",
    "\t\t\t\t\t\t\t'intent_llm': intent_llm,\n",
    "\t\t\t\t\t\t\t'params_llm': params_llm,\n",
    "\t\t\t\t\t\t\t'run_n': run_n,\n",
    "\t\t\t\t\t\t\t'n_steps': n_steps,\n",
    "\t\t\t\t\t\t\t'n_valid': n_valid,\n",
    "\t\t\t\t\t\t\t'perc_complete': n_steps / steps_per_tcase[tcase],\n",
    "\t\t\t\t\t\t\t'perc_valid': n_valid / steps_per_tcase[tcase]\n",
    "\t\t\t\t\t\t\t}, index=[0])], ignore_index=True)\n",
    "\t\t\t\tif not tcase_results.empty:\n",
    "\t\t\t\t\tresults_summary = pd.concat([results_summary, pd.DataFrame({\n",
    "\t\t\t\t\t\t'intent_llm': intent_llm,\n",
    "\t\t\t\t\t\t'params_llm': params_llm,\n",
    "\t\t\t\t\t\t'tcase': tcase,\n",
    "\t\t\t\t\t\t'mode': 'FREYR',\n",
    "\t\t\t\t\t\t'bootstrap': with_bootstrap,\n",
    "\t\t\t\t\t\t'avg_n_steps': tcase_results['n_steps'].mean(),\n",
    "\t\t\t\t\t\t'std_n_steps': tcase_results['n_steps'].std(),\n",
    "\t\t\t\t\t\t'n_steps_ci_hi': get_credible_interval(tcase_results['n_steps'])[0],\n",
    "\t\t\t\t\t\t'n_steps_ci_lo': get_credible_interval(tcase_results['n_steps'])[1],\n",
    "\t\t\t\t\t\t'avg_n_valid': tcase_results['n_valid'].mean(),\n",
    "\t\t\t\t\t\t'std_n_valid': tcase_results['n_valid'].std(),\n",
    "\t\t\t\t\t\t'n_valid_ci_hi': get_credible_interval(tcase_results['n_valid'])[0],\n",
    "\t\t\t\t\t\t'n_valid_ci_lo': get_credible_interval(tcase_results['n_valid'])[1],\n",
    "\t\t\t\t\t\t'avg_perc_complete': tcase_results['perc_complete'].mean(),\n",
    "\t\t\t\t\t\t'std_perc_complete': tcase_results['perc_complete'].std(),\n",
    "\t\t\t\t\t\t'perc_complete_ci_hi': get_credible_interval(tcase_results['perc_complete'])[0],\n",
    "\t\t\t\t\t\t'perc_complete_ci_lo': get_credible_interval(tcase_results['perc_complete'])[1],\n",
    "\t\t\t\t\t\t'avg_perc_valid': tcase_results['perc_valid'].mean(),\n",
    "\t\t\t\t\t\t'std_perc_valid': tcase_results['perc_valid'].std(),\n",
    "\t\t\t\t\t\t'perc_valid_ci_hi': get_credible_interval(tcase_results['perc_valid'])[0],\n",
    "\t\t\t\t\t\t'perc_valid_ci_lo': get_credible_interval(tcase_results['perc_valid'])[1],\n",
    "\t\t\t\t\t}, index=[0])], ignore_index=True)\n",
    "\treturn results_summary\n",
    "\n",
    "def get_summary_tool(df: pd.DataFrame,\n",
    "                     with_bootstrap: bool) -> pd.DataFrame:\n",
    "\tresults_summary = pd.DataFrame()\n",
    "\tfor intent_llm in llms:\n",
    "\t\tfor tcase in tcases:\n",
    "\t\t\ttcase_results = pd.DataFrame()\n",
    "\t\t\tfor run_n in df['run_n'].unique():\n",
    "\t\t\t\ttest_results = df.loc[(df['intent_llm'] == intent_llm) &\n",
    "\t\t\t\t\t\t\t\t\t  (df['run_n'] == run_n) &\n",
    "\t\t\t\t\t\t\t\t\t  (df['test_case'] == tcase)]\n",
    "\t\t\t\tif not test_results.empty:\n",
    "\t\t\t\t\tif not with_bootstrap:\n",
    "\t\t\t\t\t\tvalid_steps = test_results[test_results['valid_design'] == True]\n",
    "\t\t\t\t\t\tn_steps = valid_steps['step'].max() if not valid_steps.empty else 0\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\tn_steps = df['step'].max()\n",
    "\t\t\t\t\tn_valid = len(test_results[test_results['valid_design'] == True])\n",
    "\t\t\t\t\ttcase_results = pd.concat([tcase_results, pd.DataFrame({\n",
    "\t\t\t\t\t\t'intent_llm': intent_llm,\n",
    "\t\t\t\t\t\t'params_llm': intent_llm,\n",
    "\t\t\t\t\t\t'run_n': run_n,\n",
    "\t\t\t\t\t\t'n_steps': n_steps,\n",
    "\t\t\t\t\t\t'n_valid': n_valid,\n",
    "\t\t\t\t\t\t'perc_complete': n_steps / steps_per_tcase[tcase],\n",
    "\t\t\t\t\t\t'perc_valid': n_valid / steps_per_tcase[tcase]\n",
    "\t\t\t\t\t\t}, index=[0])], ignore_index=True)\n",
    "\t\t\tif not tcase_results.empty:\n",
    "\t\t\t\tresults_summary = pd.concat([results_summary, pd.DataFrame({\n",
    "\t\t\t\t\t'intent_llm': intent_llm,\n",
    "\t\t\t\t\t'params_llm': intent_llm,\n",
    "\t\t\t\t\t'tcase': tcase,\n",
    "\t\t\t\t\t'mode': 'Tool',\n",
    "\t\t\t\t\t'bootstrap': with_bootstrap,\n",
    "\t\t\t\t\t'avg_n_steps': tcase_results['n_steps'].mean(),\n",
    "\t\t\t\t\t'std_n_steps': tcase_results['n_steps'].std(),\n",
    "\t\t\t\t\t'n_steps_ci_hi': get_credible_interval(tcase_results['n_steps'])[0],\n",
    "\t\t\t\t\t'n_steps_ci_lo': get_credible_interval(tcase_results['n_steps'])[1],\n",
    "\t\t\t\t\t'avg_n_valid': tcase_results['n_valid'].mean(),\n",
    "\t\t\t\t\t'std_n_valid': tcase_results['n_valid'].std(),\n",
    "\t\t\t\t\t'n_valid_ci_hi': get_credible_interval(tcase_results['n_valid'])[0],\n",
    "\t\t\t\t\t'n_valid_ci_lo': get_credible_interval(tcase_results['n_valid'])[1],\n",
    "\t\t\t\t\t'avg_perc_complete': tcase_results['perc_complete'].mean(),\n",
    "\t\t\t\t\t'std_perc_complete': tcase_results['perc_complete'].std(),\n",
    "\t\t\t\t\t'perc_complete_ci_hi': get_credible_interval(tcase_results['perc_complete'])[0],\n",
    "\t\t\t\t\t'perc_complete_ci_lo': get_credible_interval(tcase_results['perc_complete'])[1],\n",
    "\t\t\t\t\t'avg_perc_valid': tcase_results['perc_valid'].mean(),\n",
    "\t\t\t\t\t'std_perc_valid': tcase_results['perc_valid'].std(),\n",
    "\t\t\t\t\t'perc_valid_ci_hi': get_credible_interval(tcase_results['perc_valid'])[0],\n",
    "\t\t\t\t\t'perc_valid_ci_lo': get_credible_interval(tcase_results['perc_valid'])[1],\n",
    "\t\t\t\t}, index=[0])], ignore_index=True)\n",
    "\treturn results_summary"
   ],
   "id": "5379514016a5df64",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print('Summary for FREYR with no bootstrap...')\n",
    "summary_freyr_nobootstrap = get_summary_freyr(freyr_nobootstrap_results, False)\n",
    "print('Summary for FREYR with bootstrap...')\n",
    "summary_freyr_bootstrap = get_summary_freyr(freyr_bootrstrap_results, True)\n",
    "print('Summary for tool usage with bootstrap...')\n",
    "summary_tool_bootstrap = get_summary_tool(tool_bootrstrap_results, True)\n",
    "print('Summary for tool usage with no bootstrap...')\n",
    "summary_tool_nobootstrap = get_summary_tool(tool_nobootstrap_results, False)\n"
   ],
   "id": "caf3cf2cb7fd811b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Concatenate all dataframes\n",
    "df = pd.concat([\n",
    "    summary_freyr_nobootstrap,\n",
    "    summary_freyr_bootstrap,\n",
    "    summary_tool_bootstrap,\n",
    "    summary_tool_nobootstrap\n",
    "], ignore_index=True)\n",
    "\n",
    "df = df.loc[((df['intent_llm'] == 'command-r') | (df['intent_llm'] == 'llama3.1') | (df['intent_llm'] == 'qwen2.5')) &\n",
    "\t                ((df['params_llm'] == df['intent_llm']) | (pd.isna(df['params_llm'])))]\n",
    "\n",
    "# Calculate credible intervals and format the required metrics\n",
    "df['n_steps_ci'] = df.apply(\n",
    "    lambda row: f\"${row['avg_n_steps']:.1f} \\pm {(row['n_steps_ci_hi'] - row['n_steps_ci_lo']) / 2:.1f}$\", axis=1)\n",
    "df['perc_complete_ci'] = df.apply(\n",
    "    lambda row: f\"${row['avg_perc_complete']:.0%} \\pm {(row['perc_complete_ci_hi'] - row['perc_complete_ci_lo']) / 2:.0%}$\", axis=1)\n",
    "df['perc_valid_ci'] = df.apply(\n",
    "    lambda row: f\"${row['avg_perc_valid']:.0%} \\pm {(row['perc_valid_ci_hi'] - row['perc_valid_ci_lo']) / 2:.0%}$\", axis=1)\n",
    "\n",
    "for (with_bootstrap, columns) in zip([False, True],\n",
    "                                   [['mode', 'intent_llm', 'params_llm', 'tcase', 'n_steps_ci', 'perc_complete_ci'],\n",
    "                                    ['mode', 'intent_llm', 'params_llm', 'tcase', 'perc_valid_ci']]):\n",
    "\tsub_df = df.loc[(df['bootstrap'] == with_bootstrap)]\n",
    "\n",
    "\t# Select the columns to display\n",
    "\tdisplay_df = sub_df[columns]\n",
    "\n",
    "\t# Sort the DataFrame for easier cell merging\n",
    "\tdisplay_df = display_df.sort_values(['mode', 'intent_llm', 'params_llm', 'tcase']).reset_index(drop=True)\n",
    "\n",
    "\theaders = {\n",
    "\t\t'mode': 'Mode',\n",
    "\t\t'intent_llm': 'Intent LLM',\n",
    "\t\t'params_llm': 'Parameters LLM',\n",
    "\t\t'tcase': 'Test Case',\n",
    "\t\t'n_steps_ci': 'Num. Steps',\n",
    "\t\t'perc_complete_ci': 'Completed Steps (\\%)',\n",
    "\t\t'perc_valid_ci': 'Valid Steps (\\%)',\n",
    "\t}\n",
    "\n",
    "\t# Generate the LaTeX table with merged cells\n",
    "\tfrom tabulate import tabulate\n",
    "\n",
    "\t# Convert to LaTeX\n",
    "\tlatex_table = tabulate(\n",
    "\t    display_df, headers=headers, tablefmt='latex_raw', showindex=False\n",
    "\t)\n",
    "\n",
    "\twith open(f'./experiments/freyr_tools_{\"single\" if with_bootstrap else \"iterative\"}_table.tex', 'w') as f:\n",
    "\t\tf.write(latex_table)"
   ],
   "id": "dd87226fe4e00171",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df",
   "id": "cbfc1178983e2ca0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Concatenate all dataframes\n",
    "df = pd.concat([\n",
    "    summary_freyr_nobootstrap,\n",
    "    summary_freyr_bootstrap,\n",
    "], ignore_index=True)\n",
    "\n",
    "# Calculate credible intervals and format the required metrics\n",
    "df['n_steps_ci'] = df.apply(\n",
    "    lambda row: f\"${row['avg_n_steps']:.1f} \\pm {(row['n_steps_ci_hi'] - row['n_steps_ci_lo']) / 2:.1f}$\", axis=1)\n",
    "df['perc_complete_ci'] = df.apply(\n",
    "    lambda row: f\"${row['avg_perc_complete']:.0%} \\pm {(row['perc_complete_ci_hi'] - row['perc_complete_ci_lo']) / 2:.0%}$\", axis=1)\n",
    "df['perc_valid_ci'] = df.apply(\n",
    "    lambda row: f\"${row['avg_perc_valid']:.0%} \\pm {(row['perc_valid_ci_hi'] - row['perc_valid_ci_lo']) / 2:.0%}$\", axis=1)\n",
    "\n",
    "for (with_bootstrap, columns) in zip([False, True],\n",
    "                                   [['mode', 'bootstrap', 'intent_llm', 'params_llm', 'tcase', 'n_steps_ci', 'perc_complete_ci'],\n",
    "                                    ['mode', 'bootstrap', 'intent_llm', 'params_llm', 'tcase', 'perc_valid_ci']]):\n",
    "\tsub_df = df.loc[(df['bootstrap'] == with_bootstrap)]\n",
    "\n",
    "\t# Select the columns to display\n",
    "\tdisplay_df = sub_df[columns]\n",
    "\n",
    "\t# Sort the DataFrame for easier cell merging\n",
    "\tdisplay_df = display_df.sort_values(['intent_llm', 'params_llm', 'tcase']).reset_index(drop=True)\n",
    "\n",
    "\t# Generate the LaTeX table with merged cells\n",
    "\tfrom tabulate import tabulate\n",
    "\n",
    "\theaders = {\n",
    "\t\t'mode': 'Mode',\n",
    "\t\t'intent_llm': 'Intent LLM',\n",
    "\t\t'params_llm': 'Parameters LLM',\n",
    "\t\t'tcase': 'Test Case',\n",
    "\t\t'n_steps_ci': 'Num. Steps',\n",
    "\t\t'perc_complete_ci': 'Completed Steps (\\%)',\n",
    "\t\t'perc_valid_ci': 'Valid Steps (\\%)',\n",
    "\t}\n",
    "\n",
    "\t# Convert to LaTeX\n",
    "\tlatex_table = tabulate(\n",
    "\t    display_df, headers=headers, tablefmt='latex_raw', showindex=False\n",
    "\t)\n",
    "\n",
    "\twith open(f'./experiments/freyr_{\"single\" if with_bootstrap else \"iterative\"}_table.tex', 'w') as f:\n",
    "\t\tf.write(latex_table)"
   ],
   "id": "86858be89c4e9a26",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "latex_table",
   "id": "a1ce5760f7199b94",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fails = pd.DataFrame()\n",
    "\n",
    "df = freyr_bootrstrap_results.loc[((freyr_bootrstrap_results['intent_llm'] == 'command-r') |\n",
    "                                       (freyr_bootrstrap_results['intent_llm'] == 'llama3.1') |\n",
    "                                       (freyr_bootrstrap_results['intent_llm'] == 'qwen2.5')) &\n",
    "\t                ((freyr_bootrstrap_results['params_llm'] == freyr_bootrstrap_results['intent_llm']) |\n",
    "\t                 (pd.isna(freyr_bootrstrap_results['params_llm'])))]\n",
    "\n",
    "for test_case in range(1, 5 + 1):\n",
    "\tfor step in range(1, steps_per_tcase[test_case] + 1):\n",
    "\t\tsub_df = df.loc[(df['test_case'] == test_case) & (df['step'] == step)]\n",
    "\t\tdomain_fails = len(sub_df.loc[sub_df['valid_domain'] == False])\n",
    "\t\tdesign_fails = len(sub_df.loc[sub_df['valid_design'] == False])\n",
    "\t\ttot = len(sub_df)\n",
    "\n",
    "\t\tfails = pd.concat([fails, pd.DataFrame({'test_case': test_case,\n",
    "\t\t                                        'step': step,\n",
    "\t\t                                        'domain_fails': domain_fails,\n",
    "\t\t                                        'design_fails': design_fails,\n",
    "\t\t                                        'tot': tot}, index=[0])], ignore_index=True)\n",
    "# Calculate percentage columns\n",
    "fails['domain_fail_pct'] = (fails['domain_fails'] / fails['tot'])\n",
    "fails['design_fail_pct'] = (fails['design_fails'] / fails['tot'])\n",
    "\n",
    "n = 5\n",
    "\n",
    "top_domain_fails = fails.nlargest(n, 'domain_fails')[['test_case', 'step', 'domain_fails', 'domain_fail_pct']]\n",
    "top_design_fails = fails.nlargest(n, 'design_fails')[['test_case', 'step', 'design_fails', 'design_fail_pct']]\n",
    "\n",
    "with open('./experiments/failing_rankings', 'w') as z:\n",
    "\tz.write('Top 5 Domain Failing\\n')\n",
    "\tfor i, (_, row) in enumerate(top_domain_fails.iterrows()):\n",
    "\t\twith open(f'./test_cases/test_case_{int(row[\"test_case\"])}', 'r') as f:\n",
    "\t\t\tquery = f.readlines()[int(row['step'])].strip()\n",
    "\n",
    "\t\tz.write(\n",
    "\t\t\tf'#{i + 1} - T{row[\"test_case\"]}.{row[\"step\"]}: {query} - {row[\"domain_fails\"]} ({row[\"domain_fail_pct\"]:.0%})\\n')\n",
    "\n",
    "\tz.write('\\n\\nTop 5 Design Failing\\n')\n",
    "\tfor i, (_, row) in enumerate(top_design_fails.iterrows()):\n",
    "\t\twith open(f'./test_cases/test_case_{int(row[\"test_case\"])}', 'r') as f:\n",
    "\t\t\tquery = f.readlines()[int(row['step'])].strip()\n",
    "\n",
    "\t\tz.write(\n",
    "\t\t\tf'#{i + 1} - T{row[\"test_case\"]}.{row[\"step\"]}: {query} - {row[\"design_fails\"]} ({row[\"design_fail_pct\"]:.0%})\\n')\n"
   ],
   "id": "903125f97dab44e3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def analyze_tokens_and_time(df: pd.DataFrame,\n",
    "                            freyr_mode: bool,\n",
    "                            with_bootstrap: bool) -> Tuple[int, int, float]:\n",
    "\tinput_tokens = []\n",
    "\toutput_tokens = []\n",
    "\ttime = []\n",
    "\n",
    "\tfor i, logfile in enumerate(df['logfile'].unique()):\n",
    "\t\twith open(f'./experiments/{\"freyr\" if freyr_mode else \"tool_v2\"}_{\"bootstrap\" if with_bootstrap else \"no_bootstrap\"}/{logfile}.log', 'r') as f:\n",
    "\t\t\tls = f.readlines()\n",
    "\n",
    "\t\t\tstep_start_idxs = [i for i, l in enumerate(ls) if 'main - step=' in l]\n",
    "\n",
    "\t\t\tfor i, step_idx in enumerate(step_start_idxs):\n",
    "\t\t\t\tin_tks_step, out_tks_step, t_step = 0, 0, 0.0\n",
    "\t\t\t\tfor j, l in enumerate(ls, start=step_idx):\n",
    "\n",
    "\t\t\t\t\t# LocalLLM.extract_intents - Prompt Tokens: 448; Completion Tokens: 6; Time: 0.2812\n",
    "\t\t\t\t\tif 'Prompt Tokens' in l:\n",
    "\t\t\t\t\t\tin_tks, out_tks, t = l.split(' - ')[1].split(';')\n",
    "\t\t\t\t\t\tin_tks_step += int(in_tks.split(':')[1])\n",
    "\t\t\t\t\t\tout_tks_step += int(out_tks.split(':')[1])\n",
    "\t\t\t\t\t\tt_step += float(t.split(':')[1])\n",
    "\n",
    "\t\t\t\t\tif i < len(step_start_idxs) - 1 and j > step_start_idxs[i + 1]:\n",
    "\t\t\t\t\t\tbreak\n",
    "\n",
    "\t\t\t\tinput_tokens.append(in_tks_step)\n",
    "\t\t\t\toutput_tokens.append(out_tks_step)\n",
    "\t\t\t\ttime.append(t_step)\n",
    "\n",
    "\treturn input_tokens, output_tokens, time\n"
   ],
   "id": "3b32ba5f098209cf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "tokens_time_dict = {}\n",
    "\n",
    "for fmode, bmode, original_df in zip([True, True, False, False],\n",
    "                                     [False, True, False, True],\n",
    "                                     [freyr_nobootstrap_results, freyr_bootrstrap_results, tool_nobootstrap_results, tool_bootrstrap_results]):\n",
    "\tprint(f'Processing {\"FREYR\" if fmode else \"Tool\"} {\"bootstrap\" if bmode else \"no-bootstrap\"}...')\n",
    "\t# sub_df = df.loc[((df['intent_llm'] == 'command-r') | (df['intent_llm'] == 'llama3.1') | (df['intent_llm'] == 'qwen2.5')) &\n",
    "\t#                 ((df['params_llm'] == df['intent_llm']) | (pd.isna(df['params_llm'])))]\n",
    "\tsub_original_df = original_df.loc[((original_df['intent_llm'] == 'command-r') | (original_df['intent_llm'] == 'llama3.1') | (original_df['intent_llm'] == 'qwen2.5')) &\n",
    "\t                ((original_df['params_llm'] == original_df['intent_llm']) | (pd.isna(original_df['params_llm'])))]\n",
    "\n",
    "\tk = f'{\"freyr\" if fmode else \"tool\"}_{\"bootstrap\" if bmode else \"nobootstrap\"}'\n",
    "\ttokens_time_dict[k] = {}\n",
    "\n",
    "\tfor intent_llm in ['command-r', 'llama3.1', 'qwen2.5']:\n",
    "\t\ttokens_time_dict[k][intent_llm] = []\n",
    "\t\tfor tn in [1,2,3,4,5]:\n",
    "\t\t\tdf = sub_original_df.loc[(sub_original_df['test_case'] == tn) &\n",
    "\t\t\t                         (sub_original_df['intent_llm'] == intent_llm) &\n",
    "\t\t\t                         (sub_original_df['params_llm'] == intent_llm if fmode else sub_original_df['params_llm'].isna())]\n",
    "\n",
    "\t\t\tin_tks, out_tks, t = analyze_tokens_and_time(df=df, freyr_mode=fmode, with_bootstrap=bmode)\n",
    "\t\t\ttokens_time_dict[k][intent_llm].append({\n",
    "\t\t\t\t'input_tokens_avg': np.mean(in_tks),\n",
    "\t\t\t\t'input_tokens_ci': get_credible_interval(pd.Series(in_tks)),\n",
    "\t\t\t\t'output_tokens_avg': np.mean(out_tks),\n",
    "\t\t\t\t'output_tokens': get_credible_interval(pd.Series(out_tks)),\n",
    "\t\t\t\t'time_avg': np.mean(t),\n",
    "\t\t\t\t'time_ci': get_credible_interval(pd.Series(t)),\n",
    "\t\t\t})\n"
   ],
   "id": "83b5aae459e9951e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import json\n",
    "\n",
    "with open('./experiments/tokens_time_dict.json', 'w') as f:\n",
    "\tjson.dump(tokens_time_dict, f)"
   ],
   "id": "739eb9f4e803fa8c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.rcParams['font.family'] = 'serif'\n",
    "plt.rcParams['font.serif'] = ['Times New Roman'] + plt.rcParams['font.serif']\n",
    "\n",
    "for tcase in range(5):\n",
    "\tfreyr = tokens_time_dict['freyr_bootstrap']\n",
    "\ttools = tokens_time_dict['tool_bootstrap']\n",
    "\n",
    "\tin_tks_freyr = {k: freyr[k][tcase]['input_tokens'] for k in freyr.keys()}\n",
    "\tin_tks_tools = {k: tools[k][tcase]['input_tokens'] for k in tools.keys()}\n",
    "\tout_tks_freyr = {k: freyr[k][tcase]['output_tokens'] for k in freyr.keys()}\n",
    "\tout_tks_tools = {k: tools[k][tcase]['output_tokens'] for k in tools.keys()}\n",
    "\ttime_freyr = {k: freyr[k][tcase]['time'] for k in freyr.keys()}\n",
    "\ttime_tools = {k: tools[k][tcase]['time'] for k in tools.keys()}\n",
    "\n",
    "\t# Extract data for plotting\n",
    "\tlabels = ['command-r', 'llama3.1', 'qwen2.5']\n",
    "\tx = np.arange(len(labels))  # X-axis positions\n",
    "\twidth = 0.4  # Width of bars\n",
    "\n",
    "\t# Plot each pair\n",
    "\tfig, ax = plt.subplots(figsize=(10, 6))\n",
    "\tfor i in range(len(in_tks_freyr)):\n",
    "\t\tax.bar(x + i * (width * 2), list(in_tks_freyr.values())[i], width, label=f'FREYR')\n",
    "\t\tax.bar(x + (i * (width * 2) + width), list(in_tks_tools.values())[i], width, label=f'Tools')\n",
    "\n",
    "\t# Customization\n",
    "\tax.set_xlabel('LLMs')\n",
    "\tax.set_ylabel('Input Tokens')\n",
    "\tax.set_title('')\n",
    "\tax.set_xticks(x + width)\n",
    "\tax.set_xticklabels(labels)\n",
    "\tax.legend(loc='best')\n",
    "\n",
    "\t# Display the plot\n",
    "\tplt.tight_layout()\n",
    "\tplt.show()\n"
   ],
   "id": "e09525474069ef69",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "raise Exception",
   "id": "9746bd91220ad633",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.rcParams['font.family'] = 'serif'\n",
    "plt.rcParams['font.serif'] = ['Times New Roman'] + plt.rcParams['font.serif']\n",
    "\n",
    "def compare_on_tcase(df: pd.DataFrame,\n",
    "                     freyr_mode: bool,\n",
    "                     with_bootstrap: bool,\n",
    "                     tn: int) -> None:\n",
    "\tk = 'avg_perc_complete' if not with_bootstrap else 'avg_perc_valid'\n",
    "\n",
    "\t# sub_df = df.loc[((df['intent_llm'] == 'command-r') | (df['intent_llm'] == 'llama3.1') | (df['intent_llm'] == 'qwen2.5')) &\n",
    "\t#                 ((df['params_llm'] == df['intent_llm']) | (pd.isna(df['params_llm'])))]\n",
    "\n",
    "\tmatrix = df.loc[(df['tcase']) == tn].pivot(index='params_llm', columns='intent_llm', values=k)\n",
    "\t\n",
    "\tplt.figure(figsize=(8, 6))\n",
    "\tplt.imshow(matrix, cmap='coolwarm', aspect='auto', vmin=0.0, vmax=1.0)\n",
    "\t\n",
    "\tplt.colorbar(label='Average % Complete' if not with_bootstrap else 'Average % Valid')\n",
    "\t\n",
    "\tplt.xticks(ticks=np.arange(len(matrix.columns)), labels=matrix.columns, rotation=45)\n",
    "\tplt.yticks(ticks=np.arange(len(matrix.index)), labels=matrix.index)\n",
    "\tplt.xlabel('Intent LLM' if freyr_mode else 'LLM')\n",
    "\tplt.ylabel('Params LLM' if freyr_mode else 'LLM')\n",
    "\tplt.title(f'Avg % {\"Complete\" if not with_bootstrap else \"Valid\"} for T{tn}')\n",
    "\t\n",
    "\tfor i in range(len(matrix.index)):\n",
    "\t\tfor j in range(len(matrix.columns)):\n",
    "\t\t\tv = matrix.iloc[i, j]\n",
    "\t\t\tif not np.isnan(v):\n",
    "\t\t\t\tplt.text(j, i, f\"{v:.2f}\", ha='center', va='center', color='white')\n",
    "\t\n",
    "\tplt.tight_layout()\n",
    "\tplt.savefig(f'./experiments/T{tn}_{\"freyr\" if freyr_mode else \"tool\"}_{\"bootstrap\" if with_bootstrap else \"nobootstrap\"}_cm.png', transparent=True)\n",
    "\tplt.close()"
   ],
   "id": "ffe0631cfe276fa5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def analyze_failures(df: pd.DataFrame,\n",
    "                     freyr_mode: bool,\n",
    "                     with_bootstrap: bool) -> None:\n",
    "\tvalue_err, key_err, type_err, other_err = 0, 0, 0, 0\n",
    "\twith_err = 0\n",
    "\tinvalid_domain, invalid_design, unexpected_intents = 0, 0, 0\n",
    "\tfailing = 0\n",
    "\tn = 0\n",
    "\n",
    "\t# sub_df = df.loc[((df['intent_llm'] == 'command-r') | (df['intent_llm'] == 'llama3.1') | (df['intent_llm'] == 'qwen2.5')) &\n",
    "\t#                 ((df['params_llm'] == df['intent_llm']) | (pd.isna(df['params_llm'])))]\n",
    "\tfor i, logfile in enumerate(df['logfile'].unique()):\n",
    "\t\twith open(f'./experiments/{\"freyr\" if freyr_mode else \"tool_v2\"}_{\"bootstrap\" if with_bootstrap else \"no_bootstrap\"}/{logfile}.log', 'r') as f:\n",
    "\t\t\tls = f.readlines()\n",
    "\t\t\t\n",
    "\t\t\tn_steps = 0\n",
    "\t\t\t\n",
    "\t\t\tvalue_errs, key_errs, type_errs = [], [], [],\n",
    "\t\t\tintent_lines, domain_lines, design_lines = [], [], []\n",
    "\t\t\tfor line in ls:\n",
    "\t\t\t\tif 'main - ValueError' in line: value_errs.append(line)\n",
    "\t\t\t\tif 'main - KeyError' in line: key_errs.append(line)\n",
    "\t\t\t\tif 'main - TypeError' in line: type_errs.append(line)\n",
    "\t\t\t\tif 'main - expected_intents' in line: intent_lines.append(line)\n",
    "\t\t\t\tif 'main - valid_domain' in line: domain_lines.append(line)\n",
    "\t\t\t\tif 'main - valid_design' in line: design_lines.append(line)\n",
    "\t\t\t\tif 'main - step=' in line: n_steps += 1\n",
    "\t\t\t\t\n",
    "\t\t\tvalue_err += len(value_errs)\n",
    "\t\t\tkey_err += len(key_errs)\n",
    "\t\t\ttype_err += len(type_errs)\n",
    "\t\t\twith_err += len(value_errs) + len(key_errs) + len(type_errs)\n",
    "\n",
    "\t\t\tif with_bootstrap and n_steps not in steps_per_tcase.values(): print(logfile)\n",
    "\n",
    "\t\t\tn += n_steps\n",
    "\n",
    "\t\t\tif freyr_mode:\n",
    "\t\t\t\tfor intent_line, domain_line, design_line in zip(intent_lines, domain_lines, design_lines):\n",
    "\t\t\t\t\tif 'False' in intent_line or 'False' in domain_line or 'False' in design_line:\n",
    "\t\t\t\t\t\tfailing += 1\n",
    "\t\t\t\t\tif 'False' in intent_line:\n",
    "\t\t\t\t\t\tunexpected_intents += 1\n",
    "\t\t\t\t\tif 'False' in domain_line:\n",
    "\t\t\t\t\t\tinvalid_domain += 1\n",
    "\t\t\t\t\tif 'False' in design_line:\n",
    "\t\t\t\t\t\tinvalid_design += 1\n",
    "\t\t\telse:\n",
    "\t\t\t\tfor domain_line, design_line in zip(domain_lines, design_lines):\n",
    "\t\t\t\t\tif 'False' in domain_line or 'False' in design_line:\n",
    "\t\t\t\t\t\tfailing += 1\n",
    "\t\t\t\t\tif 'False' in domain_line:\n",
    "\t\t\t\t\t\tinvalid_domain += 1\n",
    "\t\t\t\t\tif 'False' in design_line:\n",
    "\t\t\t\t\t\tinvalid_design += 1\n",
    "\t\t\t\n",
    "\twith open(f'./experiments/{\"FREYR\" if freyr_mode else \"Tool\"}_{\"bootstrap\" if with_bootstrap else \"no-bootstrap\"}_failures.txt', 'w') as f:\n",
    "\t\tf.write(f'Total failing: {failing} / {n} ({failing / n:.2%})\\n' if failing != 0 else 'Total failing: : N/A\\n')\n",
    "\t\tif freyr_mode:\n",
    "\t\t\tf.write(f'Total wrong intents: {unexpected_intents} / {failing} ({unexpected_intents / failing:.2%})\\n' if failing != 0 else 'Total wrong intents: N/A\\n')\n",
    "\t\tf.write(f'Total domain errors: {invalid_domain} / {failing} ({invalid_domain / failing:.2%})\\n' if failing != 0 else 'Total domain/design errors: N/A\\n\\n')\n",
    "\t\tf.write(f'Total design errors: {invalid_design} / {failing} ({invalid_design / failing:.2%})\\n\\n' if failing != 0 else 'Total domain/design errors: N/A\\n\\n')\n",
    "\t\tf.write(f'Total errors: {with_err} / {n} ({with_err / n:.2%})\\n' if n != 0 else 'ValueErrors: N/A\\n')\n",
    "\t\tf.write(f'ValueErrors: {value_err} / {with_err} ({value_err / with_err:.2%})\\n' if with_err != 0 else 'ValueErrors: N/A\\n')\n",
    "\t\tf.write(f'KeyErrors: {key_err} / {with_err} ({key_err / with_err:.2%})\\n' if with_err != 0 else 'KeyErrors: N/A\\n')\n",
    "\t\tf.write(f'TypeErrors: {type_err} / {with_err} ({type_err / with_err:.2%})\\n' if with_err != 0 else 'TypeErrors: N/A\\n')"
   ],
   "id": "11f6442f02f52049",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for df, fmode, bmode, original_df in zip([summary_freyr_nobootstrap, summary_freyr_bootstrap, summary_tool_nobootstrap, summary_tool_bootstrap],\n",
    "                                         [True, True, False, False],\n",
    "                                         [False, True, False, True],\n",
    "                                         [freyr_nobootstrap_results, freyr_bootrstrap_results, tool_nobootstrap_results, tool_bootrstrap_results]):\n",
    "\tprint(f'Processing {\"FREYR\" if fmode else \"Tool\"} {\"bootstrap\" if bmode else \"no-bootstrap\"}...')\n",
    "\t# sub_df = df.loc[((df['intent_llm'] == 'command-r') | (df['intent_llm'] == 'llama3.1') | (df['intent_llm'] == 'qwen2.5')) &\n",
    "\t#                 ((df['params_llm'] == df['intent_llm']) | (pd.isna(df['params_llm'])))]\n",
    "\t# sub_original_df = original_df.loc[((original_df['intent_llm'] == 'command-r') | (original_df['intent_llm'] == 'llama3.1') | (original_df['intent_llm'] == 'qwen2.5')) &\n",
    "\t#                 ((original_df['params_llm'] == original_df['intent_llm']) | (pd.isna(original_df['params_llm'])))]\n",
    "\tsub_df = df\n",
    "\tsub_original_df = original_df\n",
    "\tprint('Creating plots...')\n",
    "\tfor tn in [1,2,3,4,5]:\n",
    "\t\tcompare_on_tcase(df=sub_df, freyr_mode=fmode, with_bootstrap=bmode, tn=tn)\n",
    "\tprint('Analyzing failures...')\n",
    "\tanalyze_failures(sub_original_df, fmode, bmode)"
   ],
   "id": "69cec73d7a3448b5",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# freyr_mode = True\n",
    "# with_bootstrap = False\n",
    "# \n",
    "# for logfile in freyr_nobootstrap_results['logfile']:\n",
    "# \twith open(f'./experiments/{\"freyr\" if freyr_mode else \"tool\"}_{\"bootstrap\" if with_bootstrap else \"no_bootstrap\"}/{logfile}.log', 'r') as f:\n",
    "# \t\tls = f.readlines()\n",
    "# \t\tif \"Exception\" in ls[-2]:\n",
    "# \t\t\tprint(logfile, ls[-2])\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "41a5146a7b24e913",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "e9139d8496727d00",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
